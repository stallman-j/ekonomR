---
title: "Raster Extraction Workflow"
layout: single
date: "2025-04-17"
output:
  pdf_document: default
  rmarkdown::html_vignette: default
toc_sticky: true
author_profile: true
toc: true
toc_label: Contents
vignette: "%\\VignetteIndexEntry{intermediate-mapping} %\\VignetteEngine{knitr::rmarkdown}
  %\\VignetteEncoding{UTF-8}\n"
---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```

**Make sure** you've got the latest version of `ekonomR`. It's getting updated frequently. 

If you're not sure if your `ekonomR` is up to date or you're new to the woods, you may want to check out the vignette [Getting Started with ekonomR](https://stallman-j.github.io/ekonomR/vignettes/getting-started-with-ekonomR/), and the [Coding Review](https://stallman-j.github.io/ekonomR/vignettes/coding-review/) for a couple key operations I'll be assuming you know.

Bring in the `ekonomR` package: install ahead of time if you need to. It brings in a bunch of packages that we'll be using.

```{r}

# uncomment if you need to install
#install.packages("remotes")
#remotes::install_github("stallman-j/ekonomR")

library(ekonomR)
```

# Agenda

Here's the agenda for today:

1. Download a year of the [Climate Hazards Center InfraREd Precipitation with Stations](https://www.chc.ucsb.edu/data/chirps-gefs) data.
2. Download some country shapefiles from [GADM](https://www.hydrosheds.org/hydroatlas).
3. Extract the total precipitation over each day in the year from the CHIRPS onto the polygon basins within our country of choice, which we'll take as Benin.

# Set paths and parameters

Let's set a couple paths and parameters

```{r, eval = TRUE, results = FALSE, echo = FALSE}
data_path <- file.path("E:","data")

```

```{r, eval = FALSE, results = FALSE}
data_path <- here::here("data")

# if you want to save the data somewhere else
# data_raw_path <- file.path("E:","data","01_raw")

```

Set some parameters up top. Let's show this example for Benin. The `equal_area_crs` is an equal-area Coordinate Reference System that works well for Africa. We'll use that throughout.

```{r, eval = TRUE}

year <- 2020
country <- "BEN"
gadm_filename       <- paste0("gadm41_",country,".gpkg")
gadm_in_path        <- file.path(data_path,"01_raw","GADM",gadm_filename)
gadm_level          <- 2 # like provinces
equal_area_crs   <- "ESRI:102022"
raster_names_substring <- "precip_mm_day"

```


# Download, Clean, and Bring in a Weekly Precipitation Raster

For the sake of illustration we'll just download and clean a week's worth of CHIRPS files in 2021, but I'll write this as a loop so you could easily extend it.

Keep in mind that this download function works for 1981 to 2021, and 2022 to present. Something funky happens in 2021. (To do: add CHIRPS download vignette).

This downloads the CHIRPS daily files at .05 degree resolution, turns them into a raster stack with the `terra` package of all the daily files in the year, and saves one version in the original CRS, and one version in our equal-area CRS.

```{r, eval = TRUE, results = FALSE}

years <- year

# if you want more years
# years <- 1981:2021

  for (year in years){
  
  start_date <- paste0(as.character(year),"-06-01")
  end_date   <- paste0(as.character(year),"-06-07")
  
  # use these instead if you want the full year. Benin's rainy season is April-July and Sep-Nov so we want to observe some action here
  
  #start_date <- paste0(as.character(year),"-01-01")
  #end_date <- paste0(as.character(year),"-12-31") # uncomment if you want the full year
  
  date_sequence <- seq(lubridate::ymd(start_date),lubridate::ymd(end_date),by = "day") %>% format("%Y.%m.%d")
  date_times_for_rast <- seq(lubridate::ymd(start_date),lubridate::ymd(end_date),by = "day") 

  filenames <- paste0("chirps-v2.0.",date_sequence,".tif.gz")
  sub_urls <- paste0(year,"/",filenames)
  
  # if you want to test if your download works or something is going wrong, restrict to just two days
  # filenames<- filenames[1:2]
  # sub_urls <- sub_urls[1:2]
  
  
  ekonomR::download_multiple_files(data_subfolder = as.character(year),
                          data_raw = file.path(data_path,"01_raw","CHIRPS"),
                          base_url = "https://data.chc.ucsb.edu/products/CHIRPS-2.0/africa_daily/tifs/p05",
                          sub_urls = sub_urls,
                          filename = filenames,
                          zip_file = FALSE)
  
  # unzip the .gz files
    
    path <- file.path(data_path,"01_raw","CHIRPS",as.character(year))
    
    files <- list.files(path)
    
    files_to_unzip <- files[stringr::str_detect(files,".gz")]
    
    
    for (file in files_to_unzip){
      
    R.utils::gunzip(file.path(path,file),
                    remove = TRUE,
                    overwrite = TRUE)
    }

  unzipped_filenames <- paste0("chirps-v2.0.",date_sequence,".tif")

  year_rast <- terra::rast(x =file.path(data_path,"01_raw","CHIRPS",as.character(year),unzipped_filenames))

  terra::time(year_rast) <- date_times_for_rast
  
  names(year_rast) <- rep(raster_names_substring,times=length(filenames))
  
  output_path    <- file.path(data_path,"03_clean","CHIRPS")
  
  output_filename <- paste0("chirps_daily_p05_",year,"_first-week.tif")
  
  if (!dir.exists(output_path)) dir.create(output_path, recursive = TRUE) # recursive lets you create any needed subdirectories
  
  # replace the -9999 into NA values
  year_rast <- terra::subst(year_rast, -9999, NA)
  
  terra::writeRaster(year_rast,
          file = file.path(output_path,output_filename),
          overwrite = TRUE)
  

  output_ea_filename <- paste0("chirps_daily_p05_",year,"_equal_area_example-week.tif")
  
  ea_rast <- terra::project(year_rast, y = equal_area_crs)
  
  terra::writeRaster(ea_rast,
                     file = file.path(output_path,output_ea_filename),
                     overwrite = TRUE)
  
  }
  
```

Now, we bring in our raster. There's something funky with the projection, so we'll just redo the projection to double-check this.

```{r}
  terra_raster <- terra::rast(file.path(data_path,"03_clean","CHIRPS",output_ea_filename)) %>%
                  terra::project(y = equal_area_crs)
                

```

Let's take a look at the first day. It's easiest to just use the `terra::plot()` feature with rasters if you just want a quick look:

```{r, eval = FALSE}

terra::plot(terra_raster[[1]])
```
But if you'd rather include the sf feature on later, it makes sense to leran the `ggplot2` way. In that case, you have to make the raster into a dataframe, and tell it to plot the x and y coordinates, and then fill with the column name:

```{r, eval = TRUE}
my_plot <- ggplot2::ggplot() + 
  ggplot2::geom_raster(data = as.data.frame(terra_raster[[1]], xy = TRUE),
                      ggplot2::aes(x = x, 
                                   y =y,
                                   fill = `raster_names_substring`)
                      )
```

```{r, eval = FALSE}
my_plot 

```

Here's my convenience wrapper for saving mapping functions.

```{r, echo = FALSE, results = "hide"}
ekonomR::ggsave_plot(output_folder = here::here("output","02_figures"),
         plotname = my_plot,
         filename = paste0("gcb_territorial_emissions_",chosen_country_name,"_plot-01.png"),
         width = 8,
         height = 6,
         dpi  = 400)
```

```{r, echo = FALSE}
url <- "https://github.com/stallman-j/ekonomR/blob/main/output/02_figures/gcb_territorial_emissions_China_plot-01.png?raw=true"
knitr::include_graphics(url)

```


# Download, Clean, and Bring in the Shapefiles

This downloads province-level shapefiles for our country of interest, examines the layers, and brings in the level we want. See the [Basic Mapping](https://stallman-j.github.io/ekonomR/vignettes/basic-mapping/) vignette for more on this.

```{r}
 
  ekonomR::download_data(data_subfolder = "GADM",
                          data_raw = file.path(data_path,"01_raw"),
                          url = paste0("https://geodata.ucdavis.edu/gadm/gadm4.1/gpkg/gadm41_",country,".gpkg"),
                          filename = gadm_filename)



```

Finally, bring in our vector shapefile:

```{r}
sf::st_layers(gadm_in_path)

vector_sf <- sf::st_read(dsn = gadm_in_path,
                            layer = paste0("ADM_ADM_",gadm_level)) %>%
                sf::st_make_valid() %>%
                sf::st_transform(crs = equal_area_crs)

```

# Extract Raster to Shapefiles

These are some parameters we might want within the `raster_extract_workflow` function for the `ekonomR` package:


```{r}
  my_func           <- "sum"
  my_weights       <- NULL

```


If you'd like to examine the CRS of each of these, here's what you do. Or if you don't really care what the CRS is, you can just have the vector transformed to the CRS of the terra raster.

```{r, eval = FALSE}
  cat(terra::crs(vector_sf),"\n")
  cat(terra::crs(terra_raster),"\n")
  
  vector_sf <- vector_sf %>% sf::st_transform(crs = terra::crs(terra_raster))

    
```

Let's set the extraction paths. I like to do this up top.

```{r}
  extracted_out_path <- file.path(data_path,"03_clean","merged",country)
  
  extracted_out_filename <- paste0(country,"_",year,"_gadm-level_",gadm_level,"_daily_precip_example.rds")
```
  
Finally, get a data frame of these

```{r}
  out_df <- raster_extract_workflow(terra_raster = terra_raster,
                              vector_sf    = vector_sf,
                              save_raster_copy=FALSE,
                              extracted_out_path  = extracted_out_path,
                              extracted_out_filename = extracted_out_filename,
                              func = my_func,
                              weights = my_weights,
                              drop_geometry = FALSE
  )
```

# Bonus: Extract to buffered centroids

As a bonus feature, suppose we instead want to extract to the buffered centroids of the polygons, rather than over the entire polygons. Here's what we would do instead.

First extract the centroids of the polygons:

```{r}


```

Then, create buffers around these centroids:

```{r}


```

Finally, do the extraction again;


```{r}


```

