---
title: "Basic Regression"
layout: single
toc: true
toc_label: "Contents"
toc_sticky: true
author_profile: true
date: "2024-11-07"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{basic-regression}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

library(sandwich)
library(modelsummary)

```


**Make sure** you've got the latest version of `ekonomR`. If you've updated the package less recently than Nov 7, 2024, you should install the latest version. You might want to uncomment one or both of the following to run the installation code. (Making sure that the `ekonomR` package is unchecked in your "Packages" tab before you re-install).

```{r, results = FALSE, eval = FALSE} 
#install.packages("remotes")
#remotes::install_github("stallman-j/ekonomR")
```


If you're not sure if your `ekonomR` is up to date or you're new to the woods, you may want to check out the vignette [Getting Started with ekonomR](https://stallman-j.github.io/ekonomR/vignettes/getting-started-with-ekonomR/), and the [Coding Review](https://stallman-j.github.io/ekonomR/vignettes/coding-review/) for a couple key operations I'll be assuming you know.

# Prerequisites

I'm also going to assume that you already know the concepts we discussed in the basic cleaning vignettes. In particular, there's a discussion regarding and a simple example of merging in [Basic Cleaning: Global Carbon Budget](https://stallman-j.github.io/ekonomR/vignettes/basic-cleaning_gcb/).

First, bring `ekonomR` into your working library.

``` {r}
library(ekonomR)
```

# Overview

We're continuing our vignette series of analyzing the evidence of a Green Kuznets curve, or the hypothesis that as a country grows richer (per capita), environmental quality first deteriorates and then improves, in the context of greenhouse gas emissions.

This is a standalone vignette, but if you'd like to see the cleaning and merging that created these datasets, check out the Basic Cleaning series in the [Vignettes](https://stallman-j.github.io/ekonomR/vignettes/vignettes/) list.

# Data


The dataset we'll be using has already merged the following datasets, and created per-capita measures for gross domestic product (GDP) and greenhouse gases (GHGs)

- population data from the [World Population Prospects](https://population.un.org/wpp/Download/Standard/MostUsed/) (WPP)
- GDP data from the [Penn World Tables](https://www.rug.nl/ggdc/productivity/pwt/?lang=en) (PWT)
- greenhouse gas data from the [Global Carbon Budget (GCB)](https://globalcarbonbudget.org/).


``` {r}
data("ghg_pop_gdp")
# Uncomment below if you'd like to browse the data a little
#names(ghg_pop_gdp)
#View(head(ghg_pop_gdp))
```

In addition, this dataset also contains a few other demographic variables like life expectancy at birth.

# Conceptual framework

In this vignette, we're going to examine the relationship between greenhouse gases per capita and GDP per capita in the **cross section**: we'll choose a year of interest, and then show our regressions of the relationship across all countries for that particular year.

We'll run a linear regression, a log-log regression, and a log-linear regression, as well as a fourth specification with a cubic and quadratic term (to allow for a particular type of non-linear relationship).

## Specifications

Equation \ref{eq:eq_1} describes a cross-section specification, showing $$\text{GHGpc}$$, greenhouse gas emissions per capita in country $$i$$ and during a particular year $$t$$, as a function of $$\text{GDPpc}$$, per-capita GDP. If we only had data for the one year, we could omit the $$t$$ subscript for simplicity.

Equation \ref{eq:eq_2} adds in a quadratic and a cubic term for GDP per capita, still within a particular year $$t$$. 

Equation \ref{eq:eq_3} instead takes $$\log(\text{GDPpc})$$ as the outcome variable and $$\log(\text{GDPpc})$$ as the regressor (a log-log regression, or an elasticity).

Equation \ref{eq:eq_4} shows a regression of how $$\text{GDPpc}$$ affects $$\log(\text{GHGpc})$$ (a log-linear regression, often called a semi-elasticity).





$$GHGpc_{i,t} = \beta_0 + \beta_1 \text{GDPpc}_{i,t} + \varepsilon_{i,t}\label{eq:eq_1}$$


$$GHGpc_{i,t} = \beta_0 + \beta_1 \text{GDPpc}_{i,t} + \beta_2 \text{GDPpc}^2_{i,t} + \beta_3 \text{GDPpc}_{i,t}^3 + \varepsilon_{i,t}\label{eq:eq_2}$$


$$\log(\text{GHGpc})_{i,t} = \beta_0 + \beta_1 \log(\text{GDPpc})_{i,t} + \varepsilon_{i,t}\label{eq:eq_3}$$

$$\log(\text{GHGpc})_{i,t} = \beta_0 + \beta_1 \text{GDPpc}_{i,t} + \varepsilon_{i,t}\label{eq:eq_4}$$



## Interpreting logarithms in equations {#logs-interpretation}

The coefficient of a logged variable is best understood as a percentage. If you're feeling rusty on this, [here's a good explanation](https://openstax.org/books/introductory-business-statistics-2e/pages/13-5-interpretation-of-regression-coefficients-elasticity-and-logarithmic-transformation).

Equation \ref{eq:eq_3}, for instance, says that for a 1% increase in GDP per capita, we should expect a $$\beta_1$$% increase in greenhouse gases per capita. Equation\ref{eq:eq_4}, on the other hand, says that for an increase in one *dollar*, we should see a $$\beta_1$$ *percent* increase in greenhouse gases per capita.

## Caveat about logarithms

There's one big issue with taking logarithms in this context: there are country-years for which the GHGs from consumption are negative. Logarithms don't exist for negative numbers (and act a little funky for numbers close to zero but positive).

For the sake of convenience for this vignette, we're going to drop the values for which GHGs from consumption are negative or zero.

**Note:** This is **not** what a fuller analysis would do without serious explanation. You would want to justify dropping these observations, consider alternative methods that allowed for skewed data with negative values, consider a different transformation of your variables, or at the very least show that your regression results weren't sensitive to the dropped values.

We're going to redefine the data here so that we keep just `gcb_ghg_territorial_pc`, or territorial emissions per capita, and can just run the regressions without warnings.

``` {r}
data <- ghg_pop_gdp %>%
        dplyr::filter(gcb_ghg_territorial_pc > 0)
```

# Regression equations

One of the things that makes regression code messy is going in and rewriting all your equations and variables. It can be easy to lose track of what regressions you've already run and what regressions you're currently running.

That's why `ekonomR` has a simple function called `reg_equation()` that turns your vectors of character variables into a formula that you can just plug into a linear model. 

It works with `lm()` (linear models) and the `feols()` (fixed effects with ordinary least squares) function in the `fixest` package so that you can define your outcome variables, regressor variables, and fixed effects variables up top.

Usually, a single table that fits on a portrait-oriented page will have some 3 to 6 columns. If you need more columns than that, you likely need to reorient your page so that it's landscape. 

If you're showing more than six equations, you should also question if these are the regressions you want to run, or if your output would look better spread across two tables, or if you're overwhelming your audience with regressions.

We'll examine the outcome variable `gcb_ghg_territorial_pc`, or GHGs from territorial emissions per capita, as per the Global Carbon Budget. Let's examine this first with `gdp_pc` or GDP per capita.

## Example Run-through 

If you're running an interactive session in the R console, here's how you might explore a regression.

This is a very simple equation. You can write a regression manually, but using `reg_equation()` is going to allow us to soft-code the variables we include earlier in the script.

``` {r}
reg_eq_ex <- ekonomR::reg_equation(outcome_var = "gcb_ghg_territorial_pc",
                                regressor_vars = c("gdp_pc"))

reg_eq_ex
```
 The `~` usually means "by" or "on" in R.
 
Let's restrict the year we're considering to 1960 so that we don't have to worry about trends over time. We'll do it by setting a parameter `cross_section_year` so that this is easy to change throughout the code.

```{r}

cross_section_year <- 1960

data_cross_section <- data %>%
                      dplyr::filter(year == cross_section_year)
```

Now we run the regression, and show the output. The standard errors we get here as a default with `summary()` are not heteroskedasticity-robust. The R package `lmtest` has a function called `coeftest()` that we can use to get robust standard errors. This is what you would get if you used `, robust` in Stata. 

If you can't remember why we care about heteroskedasticity (or what heteroskedasticity is), I recommend checking out [Ben Lambert's Youtube summary of heteroskedasticity](https://www.youtube.com/watch?v=zRklTsY9w9c&list=PLwJRxp3blEvZyQBTTOMFRP_TDaSdly3gU&index=54) and the next couple videos in that playlist.

There's going to be a really easy way to put heteroskedasticity-robust standard errors into our final table, but it's nice to know the commands for digging into this manually. If I'm exploring a dataset, I often run the following commands in my console to see how the output's looking before I start making my table.


```{r}
lm_example <- stats::lm(reg_eq_ex, 
                        data = data_cross_section)

summary(lm_example)

lmtest::coeftest(lm_example,   vcov = vcovHC, type = "HC1")
```
Unsurprisingly, the heteroskedasticity-robust standard errors are a little bigger.

## Multiple regression equations

Now that we know how to go back and examine the output, let's generate all four of our regression equations in one go.

To get the squared and cubic terms, we use `I(varname^2)` and `I(varname^3)` respectively. You could use `poly(varname,2)` but that's hard to interpret.

```{r}
reg_eq_1 <- ekonomR::reg_equation(outcome_var = "gcb_ghg_territorial_pc",
                                  regressor_vars = c("gdp_pc"))
reg_eq_2 <- ekonomR::reg_equation(outcome_var = "log(gcb_ghg_territorial_pc)",
                                  regressor_vars = c("log(gdp_pc)"))
reg_eq_3 <- ekonomR::reg_equation(outcome_var = "log(gcb_ghg_territorial_pc)",
                                  regressor_vars = c("gdp_pc"))
reg_eq_4 <- ekonomR::reg_equation(outcome_var = "gcb_ghg_territorial_pc",
                                  regressor_vars = c("gdp_pc","I(gdp_pc^2)","I(gdp_pc^3)"))

# display
reg_eq_1
reg_eq_2
reg_eq_3
reg_eq_4

```
Now let's make our `lm()` objects. That is, let's actually run the regressions, keeping in mind this caveat about the robust standard errors not being quite right.

```{r}
lm_1 <- lm(reg_eq_1 , data = data_cross_section)
lm_2 <- lm(reg_eq_2 , data = data_cross_section)
lm_3 <- lm(reg_eq_3 , data = data_cross_section)
lm_4 <- lm(reg_eq_4 , data = data_cross_section)

```
We then put the corresponding models into a list called `models`. 

### Lists in R

In R, a `list` is a nice way to store objects together. An `object` in R can be just about anything, including output from a plot, a scalar, a name like `"Bob"`, a matrix, a data frame, the output from our linear regression models, or a network graph.

Here, the output of a linear regression model is an object that contains things like the standard errors, the residuals, the p values, etc. 

`models` just collects all four of those outputs together so that we can input them all into a function, called `modelsummary()`, that's going to be able to use that information and generate our output table.

In the `list` named `models`, the first item is called `"(1)"`, and it's the object generated by running an `lm()` on the regression equation `reg_eq_1` which was our simple greenhouse gases per capita on GDP per capita. 

```{r}
models <- list(
  "(1)" = lm_1,
  "(2)" = lm_2,
  "(3)" = lm_3,
  "(4)" = lm_4
)
```

Here's the simplest output with `modelsummary()`.

```{r, eval = FALSE}
modelsummary::modelsummary(models)
```

```{r, eval = TRUE, echo = FALSE}
modelsummary::modelsummary(models,
                           output = "kableExtra",
                           escape = FALSE)
            
```

Immediately we see something's up: all the GDP per capita values look like zeros to three significant digits. But wasn't the regression output significant?

```{r}
summary(lm_1)
```

The coefficient on `gdp_pc` is $$1.96\times 10^{-4}$$. It's just scaled too small!

Fortunately, in the [Basic Merging](https://stallman-j.github.io/ekonomR/vignettes/basic-merging/) vignette we created a variable called `gdp000_pc`, or GDP per capita in *thousands* of chained 2017 PPP USD.

If this weren't a vignette and I were working in my own script, I would go back and just change the regressor variables defined in `reg_eq_1` to `reg_eq_4` to be `gdp000_pc` where it was relevant. We won't need to make this change for the logarithm, since that shows up fine.

# Re-running the regression setup

Rather than manually writing the regression equations, let's put the outcome and regression variables into lists. I've also changed the order of equations (mostly for aesthetic purposes in the final table).

```{r}
reg_1_vars <- list(outvar = "gcb_ghg_territorial_pc",
                  regvars = c("gdp000_pc"))

reg_2_vars <- list(outvar = "gcb_ghg_territorial_pc",
                  regvars = c("gdp000_pc","I(gdp000_pc^2)","I(gdp000_pc^3)"))

reg_3_vars <- list(outvar = "log(gcb_ghg_territorial_pc)",
                  regvars = c("log(gdp_pc)"))

reg_4_vars <- list(outvar = "log(gcb_ghg_territorial_pc)",
                  regvars = c("gdp000_pc"))

```

Now when I want to refer to my outcome variable for, say, the second regression, and then the regressors, I can do that with the following:

```{r}
reg_2_vars$outvar
reg_2_vars$regvars
```

Now we can plug and play.

First set up our equations.

```{r}
reg_eq_1 <- ekonomR::reg_equation(outcome_var    = reg_1_vars$outvar,
                                  regressor_vars = reg_1_vars$regvars)

reg_eq_2 <- ekonomR::reg_equation(outcome_var    = reg_2_vars$outvar,
                                  regressor_vars = reg_2_vars$regvars)

reg_eq_3 <- ekonomR::reg_equation(outcome_var    = reg_3_vars$outvar,
                                  regressor_vars = reg_3_vars$regvars)

reg_eq_4 <- ekonomR::reg_equation(outcome_var    = reg_4_vars$outvar,
                                  regressor_vars = reg_4_vars$regvars)
```

Now make the regression models.
```{r}
lm_1 <- lm(reg_eq_1 , data = data_cross_section)
lm_2 <- lm(reg_eq_2 , data = data_cross_section)
lm_3 <- lm(reg_eq_3 , data = data_cross_section)
lm_4 <- lm(reg_eq_4 , data = data_cross_section)

# uncomment if you want to examine the output
#summary(lm_1)
#summary(lm_2)
#summary(lm_3)
#summary(lm_4)
```

Now put the regression models together into another list called `models`.
```{r}
models <- list(
  "(1)" = lm_1,
  "(2)" = lm_2,
  "(3)" = lm_3,
  "(4)" = lm_4)

```

Now we tell `modelsummary` to give us a table.

```{r, eval = FALSE}
modelsummary::modelsummary(models)

```

```{r, eval = TRUE, echo = FALSE}
modelsummary::modelsummary(models,
                           output = "kableExtra",
                           escape = FALSE)
            
```

We're getting there!


## Interpreting coefficients {#interpreting-coefficients}

Here's a good rule of thumb for interpreting coefficients.

If you're given the standard errors listed in parentheses, the coefficient is roughly significant at the 95% level if multiplying the thing in parentheses by 2 and then adding it to or subtracting it from the coefficient above it does not get that coefficient from negative to positive and vice versa.

In other words, we're at about 95% confidence that the coefficient isn't zero if zero isn't in the confidence interval we construct by adding two standard errors to and subtracting two standard errors from our point estimate.

For instance, the coefficient on `gdp000_pc` in column (1) is 0.196, and the standard error currently reported is 0.016, or about 0.02 (rounding up for ease of mental math). 

Double that to get 0.04.

If we add that 0.04 to 0.196, we get about 0.236, which did not cross the line to get negative (which means we didn't hit zero). 

If we subtract 0.04 from 0.196, we get 0.156, which also doesn't cross over into negative territory. This coefficient is highly significant.

On the other hand, when we do the same thing for `gdp000_pc` in column (4), we can get the coefficient of -0.077 to cross over zero and turn positive if we add $$2\times 0.116$$ to it, so this coefficient is *not* significant at the 95% level.

## Making prettier tables

This output is fine, but it generates a ton of extra rows that we don't need to show. 

We could tidy this up a whole lot by adding a title and notes, omitting things we don't typically look at, and making the variable names nicer to look at.

If you're looking for maximum customizability, check out the [modelsummary vignettes](https://modelsummary.com/vignettes/modelsummary.html). We'll just focus on the main things here.

Let's state the title, notes and coefficient labels all together. Note that we made use of `paste0()` to soft-code the cross-section year (so that we could easily check this regression for, say, 2018 instead of 1960). 

The part that says `\\label{tab:basic_reg_table}` is the way that LaTex will be able to cross-reference the table. If you're outputting to Word, it's going to get ignored, but you can delete it for simplicity. The `$^2$` formatting doesn't show up well on this web page with the current settings, but in both Word and Latex will come up as something like $$(\text{GDP pc})^2$$

```{r}
my_title <- paste0("Cross Section GHG and GDP per capita relationship, ", cross_section_year, " \\label{tab:basic_reg_table}")
  
table_notes <- "Robust standard errors given in parentheses. Population data are obtained from UN-DESA (2023). Gross domestic product (GDP) in 2017 chained PPP thousand USD per capita (PWT 2023). Greenhouse gases in tonnes of carbon per year from GCB (2024)."
  
cov_labels <- c("Intercept","GDP pc","(GDP pc)$^2$", "(GDP pc)$^3$","Log(GDP pc)")

```

### Robust standard errors
We also get the heteroskedasticity-robust standard errors with the simple option of `vcov = "HC1"` in the code below. `gof_omit` is an option that allows us to omit certain **g**oodness **o**f **f**it statistics. 

To learn more about your choices for standard errors, you can type `?modelsummary` and then search for `vcov`.

```{r reg_output_simple, eval = FALSE}
modelsummary::modelsummary(models,
             stars = FALSE,
             vcov = "HC1",
             coef_rename = cov_labels,
             title = my_title,
             format = 'latex',
             gof_omit = "AIC|BIC|RMSE|Log.Lik|Std.Errors",
             escape = FALSE
)
```

```{r, eval = TRUE, echo = FALSE}

modelsummary::modelsummary(models,
             stars = FALSE,
             vcov = "HC1",
             coef_rename = cov_labels,
             title = my_title,
             output = "kableExtra",
             gof_omit = "AIC|BIC|RMSE|Log.Lik|Std.Errors",
             escape = FALSE
)
            
```

### Adding dependent variable means

It's also common to want to put the mean of the dependent variable in our table, so that we can interpret the coefficients relative to the output values. This tells us about what's sometimes called **economic significance** (as opposed to **statistical significance**), i.e. the answer to the question, "Is the *magnitude* of the coefficients something I should care about?". 

I've muddled around a bit with how this works, and I've come up with an okay formula for somewhat automating it. 

This is why we did that weird thing with soft-coding the outcome and regressor variables, by the way. Here's how we'll do it.

1. Get the total number of regressor variables
2. Make a data frame with the means of the outcome variables. It's going to be called `rows`
3. Using the total regressor variables, make a little hack that determines where this data frame is going to be inputted into our `modelsummary` table. We want it to be just under or just above the row that's called "Num.Obs". 
4. Slot the `rows` data frame in where we want by listing it as an option to the `modelsummary()` output.

**Step 1:** 

Get the number of unique regressors. That's going to determine where to put the row with the column means, because we want it to go right after the number of observations

```{r}
n_total_regvars <- length(unique(c(reg_1_vars$regvars,reg_2_vars$regvars,reg_3_vars$regvars,reg_4_vars$regvars)))
            
```


**Step 2:** 

Create a data frame that's got the same formatting as our `modelsummary()` output table, so that we can basically just tell `modelsummary()` that in a particular spot, insert our new data frame `rows` as extra rows.

For each cell, we're going to want the mean of the dependent variable of the regression column it corresponds to.

What's this code doing:

- Start with `data_cross_section`
- Choose the column that corresponds to `reg_1_vars$outvar` (which is a fancy way of saying `"ghg_territorial_pc"`, but soft-coded so that we can make changes )
- Take the mean
- Round that average to two decimal places so it fits nicely in the table.

If you're modifying this code for your own regressions and not using logged variables, you'll want to use the following code.

``` {r, eval = FALSE}
rows <- data.frame("term" = c("Mean"),
                   "(1)"  = c(round(mean(data_cross_section[[reg_1_vars$outvar]]),2)),
                   "(2)"  = c(round(mean(data_cross_section[[reg_2_vars$outvar]]),2)),
                   "(3)"  = c(round(mean(data_cross_section[[reg_3_vars$outvar]]),2)),
                   "(4)"  = c(round(mean(data_cross_section[[reg_4_vars$outvar]]),2)))
```


That said, we're going to cheat a little bit and just use the units value for this from regression 1 for the first three columns.

That's because `log(gcb_ghg_territorial_pc)` isn't an actual variable. If we tried to do this directly, we'd get an error.

It's also not super helpful to know what the value of log greenhouse gases are, so we'll just use the levels. This is the thing that actually gets run in this particular case:


``` {r}
rows <- data.frame("term" = c("Mean"),
                   "(1)"  = c(round(mean(data_cross_section[[reg_1_vars$outvar]]),2)),
                   "(2)"  = c(round(mean(data_cross_section[[reg_1_vars$outvar]]),2)),
                   "(3)"  = c(round(mean(data_cross_section[[reg_1_vars$outvar]]),2)),
                   "(4)"  = c(round(mean(data_cross_section[[reg_1_vars$outvar]]),2)))

```
Let's also examine what `rows` is as a data frame. It'll look like the following, and we'll just slot that second row in.

| term | X.1 | X.2 | X.3 | X.4 |
|------|-----|-----|-----|-----|
| Mean | 0.59| 0.59| 0.59| 0.59|

**Step 3:**

Trial and error and some googling suggests the following funky hack will let us put the dependent variable means in the right spot.  

Why: we want `rows` to be inserted at the row that's below all the regressor variables (which take up two rows, one for the point estimate and one for the standard error), as well as the column names and the number of observations.

If you make large modifications to your own table, you might need to play with this formula a little bit to get it where you want it.

```{r}
attr(rows, 'position') <- c(2*n_total_regvars+4)            
```


**Step 4:**

Let's see the output now. We've just added the `add_rows = rows` to slot our guy in, and also added the `table_notes` defined earlier.

```{r reg_output_2, eval = FALSE}
modelsummary::modelsummary(models,
             stars = FALSE,
             vcov = "HC1",
             coef_rename = cov_labels,
             title = my_title,
             format = 'latex',
             add_rows = rows,
             gof_omit = "AIC|BIC|RMSE|Log.Lik|Std.Errors",
             escape = FALSE,
             notes = table_notes
)
```

```{r, eval = TRUE, echo = FALSE}

modelsummary::modelsummary(models,
             stars = FALSE,
             vcov = "HC1",
             coef_rename = cov_labels,
             title = my_title,
             output = "kableExtra",
             add_rows = rows,
             gof_omit = "AIC|BIC|RMSE|Log.Lik|Std.Errors",
             escape = FALSE,
             notes = table_notes

)
            
```

### Adding dependent variable names {#depvar_names}

Finally, let's add a header to state which is the dependent variable. We use `tinytable`'s `group_tt` to put the names at particular columns, which we indicate by `j` in the original table. The below output says to put `"GHGpc"` at columns 2 and 3, and `"log(GHGpc)"` in columns 4 and 5 (because column 1 corresponds to the blank column with the variable names).

```{r, eval = FALSE}
modelsummary::modelsummary(models,
             stars = FALSE,
             vcov = "HC1",
             coef_rename = cov_labels,
             title = my_title,
             format = "latex",
             add_rows = rows,
             gof_omit = "AIC|BIC|RMSE|Log.Lik|Std.Errors",
             escape = FALSE,
             notes = table_notes
)   %>%
  tinytable::group_tt(j = list("GHGpc" =2:3, "log(GHGpc)"=4:5))
```

```{r, eval = TRUE, echo = FALSE}

modelsummary::modelsummary(models,
             stars = FALSE,
             vcov = "HC1",
             coef_rename = cov_labels,
             title = my_title,
             output = "kableExtra",
             add_rows = rows,
             gof_omit = "AIC|BIC|RMSE|Log.Lik|Std.Errors",
             escape = FALSE,
             notes = table_notes

) %>%
  kableExtra::add_header_above(c(" "=1, "GHGpc" =2, "log(GHGpc)"=2))

            
```

### Output

This looks good! Now we want to output, either to LaTex or to Word. Fortunately, it's super simple to do either with just a change of options in `tinytable::save_tt()` because `tinytable` recognizes what options you wanted based on the file extension you list.

We're going to now save our results as `my_table`, so that we can refer to it directly when we're saving it as output.

```{r}
my_table <- modelsummary::modelsummary(models,
             stars = FALSE,
             vcov = "HC1",
             coef_rename = cov_labels,
             title = my_title,
             format = "latex",
             add_rows = rows,
             gof_omit = "AIC|BIC|RMSE|Log.Lik|Std.Errors",
             escape = FALSE,
             notes = table_notes
)   %>%
  tinytable::group_tt(j = list("GHGpc" =2:3, "log(GHGpc)"=4:5))

```

```{r, eval = FALSE}
  tinytable::save_tt(my_table,
                     output = here::here("output","01_tables","basic_regression_table.tex"),
                     overwrite = TRUE)
  
   tinytable::save_tt(my_table,
                     output = here::here("output","01_tables","basic_regression_table.docx"),
                     overwrite = TRUE)
```
If you get an output error like `cannot open the connection`, make sure the file is closed on your local computer before running the command. R can't overwrite the file if it's open.


```{r, eval = TRUE, echo = FALSE}
  tinytable::save_tt(my_table,
                     output = here::here("example-project","output","01_tables","basic_regression_table.tex"),
                     overwrite = TRUE)
  
   tinytable::save_tt(my_table,
                     output = here::here("example-project","output","01_tables","basic_regression_table.docx"),
                     overwrite = TRUE)
```

#### Outputting to LaTex

Do **not** copy and paste latex output directly into LaTex or Overleaf. There's a much nicer way to do it.

If you're using Overleaf, make sure you have the following code in your preamble:

``` {r, eval = FALSE}
\usepackage{tabularray}
\usepackage{float}
\usepackage{graphicx}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\UseTblrLibrary{booktabs}
\NewTableCommand{\tinytableDefineColor}[3]{\definecolor{#1}{#2}{#3}}
\newcommand{\tinytableTabularrayUnderline}[1]{\underline{#1}}
\newcommand{\tinytableTabularrayStrikeout}[1]{\sout{#1}}
```

The code `tinytable::save_tt()` will have given you a `.tex` file.

Now, you can drag and drop the entire table `.tex` file into, preferably, a folder in your Overleaf project called something like `Tables`. 

In the [Overleaf ECON 412 folder](https://www.overleaf.com/read/wsrdjdckwmbz#f4467b), for instance, I've uploaded the table from this exercises into the path `tables/tables-from-r/basic_regression_table.tex`. 

When you update your table, you just have to update the drag and drop.

If you'd like to be even fancier, you can sync your Overleaf with Dropbox or Github so that the tables update automatically when you run your R code, and then again when you compile in Overleaf.

Because we set a label above, we can also cross-reference the table. An example of how to do this is given in the file `templates/examples/inserting-tables.tex`.

# Exercises

1. In the section on [interpreting logarithms](#logs-interpretation) we described the interpretation of the coefficient $$\beta_1$$ for Equations \ref{eq:eq_2} and \ref{eq:eq_3}. State the corresponding interpretation of $$\beta_1$$ in the equation: $$ \text{GHGpc}_{i,t} = \beta_0 + \beta_1 \log(\text{GDPpc})_{i,t} + \varepsilon_{i,t}$$

2. Determine whether the coefficients in column (2) in the final table are statistically significant at the 95% level as we did in the section [interpreting coefficients](#interpreting-coefficients) by doing a rough back-of-the-envelope calculation. Round as you need.  (This is just to practice the heuristic we typically use for reading regression tables.)

3. Examine the `lm()` model output on the coefficients (intercept and `log(gdp_pc)` for the regression given by $$\log(\text{GHGpc})_{i,t} = \beta_0 + \beta_1 \log(\text{GDPpc})_{i,t} + \varepsilon_{i,t}$$ by using `summary()`. What is the outputted p value for each of the two coefficients?

4. In a *different* way than using `modelsummary()`, what are the heteroskedasticity-robust standard errors for the intercept and `log(gdp_pc)` in the regression given by $$\log(\text{GHGpc})_{i,t} = \beta_0 + \beta_1 \log(\text{GDPpc})_{i,t} + \varepsilon_{i,t}$$ ? 

    - Write the command you used. (**Hint:** See what we did with `lmtest::coeftest()`). 
    - Verify that they're the same (up to rounding error) as the output we got from the `modelsummary()` output that used `"HC1"` standard errors.
    
5. In [Adding dependent variable names](#depvar_names), change the arguments in `tinytable:group_tt()` so that `"GDPpc"` is *repeated* in columns 2 and columns 3 (that is, over the columns labeled (1) and (2)) rather than spanning *across* columns 2 and columns 3.

6. Make a conjecture about whether the coefficients will have a stronger or weaker relationship for a more recent year. That is, do you think that in more recent years the relationship between GDP per capita and greenhouse gases per capita is stronger, weaker, or about the same as it was in 1960? Briefly explain your reasoning. 

    - I'm not interested in whether you're right or wrong here. It's just good practice to document your hypotheses *before* you run an analysis.
    - If you haven't thought about your hypothesis before you do your analysis, it's easy to get off track.

7. **Make sure you do Exercise 6 before you run this analysis**. *Without* changing your conjecture ex post, re-create the analysis we did above to produce a final table with a year more recent than 2000. Compare this with your conjecture from Exercise 6. 

    - Did your reasoning hold? 
    - If you were wrong, suggest why the results might differ from your initial hypothesis.

# Just the code, please

Here's the code that you would want to modify if you're doing this for your own project.
``` {r, eval = FALSE}

# ---- Setup

# bring in ekonomR
library(ekonomR)

# bring in data
data("ghg_pop_gdp")

names(ghg_pop_gdp)
#View(head(ghg_pop_gdp))

# Things you would change in a four-column reg table without fixed effects ----
# set parameters
cross_section_year <- 1960

my_title <- paste0("Cross Section GHG and GDP per capita relationship, ", cross_section_year, " \\label{tab:basic_reg_table}")

table_notes <- "Robust standard errors given in parentheses. Population data are obtained from UN-DESA (2023). Gross domestic product (GDP) in 2017 chained PPP thousand USD per capita (PWT 2023). Greenhouse gases in tonnes of carbon per year from GCB (2024)."

cov_labels <- c("Intercept","GDP pc","(GDP pc)$^2$", "(GDP pc)$^3$","Log(GDP pc)")


# make data restrictions

# for now since we're using logs, just restrict emissions to being positive
data <- ghg_pop_gdp %>%
  dplyr::filter(gcb_ghg_territorial_pc > 0)

data_cross_section <- data %>%
  dplyr::filter(year == cross_section_year)


# define regression outcome and regressor variables
reg_1_vars <- list(outvar = "gcb_ghg_territorial_pc",
                   regvars = c("gdp000_pc"))

reg_2_vars <- list(outvar = "gcb_ghg_territorial_pc",
                   regvars = c("gdp000_pc","I(gdp000_pc^2)","I(gdp000_pc^3)"))

reg_3_vars <- list(outvar = "log(gcb_ghg_territorial_pc)",
                   regvars = c("log(gdp_pc)"))

reg_4_vars <- list(outvar = "log(gcb_ghg_territorial_pc)",
                   regvars = c("gdp000_pc"))

reg_vars_list <- list(reg_1_vars,
                      reg_2_vars,
                      reg_3_vars,
                      reg_4_vars)
# things you would not need to change ----

# make regression formulas
reg_eq_1 <- ekonomR::reg_equation(outcome_var    = reg_1_vars$outvar,
                                  regressor_vars = reg_1_vars$regvars)

reg_eq_2 <- ekonomR::reg_equation(outcome_var    = reg_2_vars$outvar,
                                  regressor_vars = reg_2_vars$regvars)

reg_eq_3 <- ekonomR::reg_equation(outcome_var    = reg_3_vars$outvar,
                                  regressor_vars = reg_3_vars$regvars)

reg_eq_4 <- ekonomR::reg_equation(outcome_var    = reg_4_vars$outvar,
                                  regressor_vars = reg_4_vars$regvars)

# make regression models
lm_1 <- lm(reg_eq_1 , data = data_cross_section)
lm_2 <- lm(reg_eq_2 , data = data_cross_section)
lm_3 <- lm(reg_eq_3 , data = data_cross_section)
lm_4 <- lm(reg_eq_4 , data = data_cross_section)

# uncomment if you want to examine the output
#summary(lm_1)
#summary(lm_2)
#summary(lm_3)
#summary(lm_4)

# put regression models into a single list
models <- list(
  "(1)" = lm_1,
  "(2)" = lm_2,
  "(3)" = lm_3,
  "(4)" = lm_4)


# add dependent variable means

# get total regressor vars so we know how many rows to skip down
n_total_regvars <- length(unique(c(reg_1_vars$regvars,reg_2_vars$regvars,reg_3_vars$regvars,reg_4_vars$regvars)))

# things you would want to check if you need to change ----
# create data frame of depvar means. 
# IMPORTANT: if you're modifying this for your own stuff, change the reg_1_vars to reg_2_vars and reg_3_vars and reg_4_vars and so forth

rows <- data.frame("term" = c("Mean"),
                   "(1)"  = c(round(mean(data_cross_section[[reg_1_vars$outvar]]),2)),
                   "(2)"  = c(round(mean(data_cross_section[[reg_1_vars$outvar]]),2)),
                   "(3)"  = c(round(mean(data_cross_section[[reg_1_vars$outvar]]),2)),
                   "(4)"  = c(round(mean(data_cross_section[[reg_1_vars$outvar]]),2)))

# say where we're going to put the rows df
attr(rows, 'position') <- c(2*n_total_regvars+4)            

# produce final output

my_table <- modelsummary::modelsummary(models,
                                       stars = FALSE,
                                       vcov = "HC1",
                                       coef_rename = cov_labels,
                                       title = my_title,
                                       format = "latex",
                                       add_rows = rows,
                                       gof_omit = "AIC|BIC|RMSE|Log.Lik|Std.Errors",
                                       escape = FALSE,
                                       notes = table_notes
)   %>%
  tinytable::group_tt(j = list("GHGpc" =2:3, "log(GHGpc)"=4:5))

# save output

# save_tt will break if you don't have the output path created. this little line below will create the path if it doesn't already exist.
# if you used ekonomR::create_folders() previously it should exist already

if (!dir.exists(here::here("output","01_tables"))) dir.create(here::here("output","01_tables"), recursive = TRUE)


tinytable::save_tt(my_table,
                   output = here::here("output","01_tables","basic_regression_table.tex"),
                   overwrite = TRUE)

tinytable::save_tt(my_table,
                   output = here::here("output","01_tables","basic_regression_table.docx"),
                   overwrite = TRUE)

```
